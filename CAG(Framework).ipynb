{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teq6LPP3bTuX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.cache_utils import DynamicCache\n",
        "import numpy as np\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "import faiss\n",
        "\n",
        "@dataclass\n",
        "class CAGConfig:\n",
        "    \"\"\"Configuration for CAG system\"\"\"\n",
        "    max_cache_size: int = 1000\n",
        "    similarity_threshold: float = 0.85\n",
        "    max_new_tokens: int = 100\n",
        "    temperature: float = 0.7\n",
        "    top_k_cache: int = 5\n",
        "    chunk_size: int = 512\n",
        "\n",
        "class CacheManager:\n",
        "    \"\"\"Advanced cache management system for CAG\"\"\"\n",
        "    def __init__(self, config: CAGConfig):\n",
        "        self.config = config\n",
        "        self.cache_store: Dict[str, DynamicCache] = {}\n",
        "        self.embeddings_index = faiss.IndexFlatL2(768)  # Using FAISS for fast similarity search\n",
        "        self.cache_keys: List[str] = []\n",
        "\n",
        "    def add_to_cache(self, key: str, cache: DynamicCache, embedding: np.ndarray):\n",
        "        \"\"\"Add a new cache entry with smart cache management\"\"\"\n",
        "        if len(self.cache_keys) >= self.config.max_cache_size:\n",
        "            # Remove least recently used cache\n",
        "            old_key = self.cache_keys.pop(0)\n",
        "            del self.cache_store[old_key]\n",
        "            self.embeddings_index.remove_ids(np.array([self.cache_keys.index(old_key)]))\n",
        "\n",
        "        self.cache_store[key] = cache\n",
        "        self.cache_keys.append(key)\n",
        "        self.embeddings_index.add(embedding.reshape(1, -1))\n",
        "\n",
        "    def find_similar_cache(self, query_embedding: np.ndarray) -> Optional[Tuple[str, DynamicCache]]:\n",
        "        \"\"\"Find most similar cache entry using FAISS\"\"\"\n",
        "        if len(self.cache_keys) == 0:\n",
        "            return None\n",
        "\n",
        "        D, I = self.embeddings_index.search(query_embedding.reshape(1, -1), 1)\n",
        "        if D[0][0] < self.config.similarity_threshold:\n",
        "            return self.cache_keys[I[0][0]], self.cache_store[self.cache_keys[I[0][0]]]\n",
        "        return None\n",
        "\n",
        "class EnhancedCAG:\n",
        "    \"\"\"Enhanced Cache-Augmented Generation system\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        config: CAGConfig = CAGConfig(),\n",
        "        device: str = None\n",
        "    ):\n",
        "        self.config = config\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initialize model and tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        self.cache_manager = CacheManager(config)\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging for monitoring cache performance\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def generate_with_cache(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        past_key_values: Optional[DynamicCache] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Enhanced generation with cache support\"\"\"\n",
        "        device = self.model.device\n",
        "        origin_len = input_ids.shape[-1]\n",
        "        input_ids = input_ids.to(device)\n",
        "        output_ids = input_ids.clone()\n",
        "        next_token = input_ids\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(self.config.max_new_tokens):\n",
        "                outputs = self.model(\n",
        "                    input_ids=next_token,\n",
        "                    past_key_values=past_key_values,\n",
        "                    use_cache=True,\n",
        "                    temperature=self.config.temperature\n",
        "                )\n",
        "\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "\n",
        "                # Apply temperature and top-k sampling\n",
        "                if self.config.temperature > 0:\n",
        "                    logits = logits / self.config.temperature\n",
        "                    filtered_logits = torch.topk(logits, k=10, dim=-1)[0]\n",
        "                    probs = torch.softmax(filtered_logits, dim=-1)\n",
        "                    next_token = torch.multinomial(probs, num_samples=1)\n",
        "                else:\n",
        "                    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "                output_ids = torch.cat([output_ids, next_token], dim=-1)\n",
        "                past_key_values = outputs.past_key_values\n",
        "\n",
        "                if self.tokenizer.eos_token_id is not None and next_token.item() == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        return output_ids[:, origin_len:]\n",
        "\n",
        "    def process_document(self, document_path: str) -> DynamicCache:\n",
        "        \"\"\"Process document and create cache\"\"\"\n",
        "        with open(document_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Chunk document for better processing\n",
        "        chunks = self._chunk_text(text)\n",
        "        cache = DynamicCache()\n",
        "\n",
        "        for chunk in chunks:\n",
        "            input_ids = self.tokenizer(chunk, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    past_key_values=cache,\n",
        "                    use_cache=True,\n",
        "                    output_hidden_states=True\n",
        "                )\n",
        "                cache = outputs.past_key_values\n",
        "\n",
        "        return cache\n",
        "\n",
        "    def _chunk_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into manageable chunks\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for word in words:\n",
        "            word_length = len(self.tokenizer.encode(word))\n",
        "            if current_length + word_length > self.config.chunk_size:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [word]\n",
        "                current_length = word_length\n",
        "            else:\n",
        "                current_chunk.append(word)\n",
        "                current_length += word_length\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "        return chunks\n",
        "\n",
        "    def answer_question(self, question: str, document_cache: DynamicCache) -> str:\n",
        "        \"\"\"Generate answer using cached knowledge\"\"\"\n",
        "        # Clean up cache to original length\n",
        "        origin_len = document_cache.key_cache[0].shape[-2]\n",
        "        self._clean_cache(document_cache, origin_len)\n",
        "\n",
        "        # Prepare question\n",
        "        input_ids = self.tokenizer(question + \"\\n\", return_tensors=\"pt\").input_ids.to(self.device)\n",
        "\n",
        "        # Generate answer\n",
        "        generated_ids = self.generate_with_cache(input_ids, document_cache)\n",
        "        answer = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def _clean_cache(self, cache: DynamicCache, origin_len: int):\n",
        "        \"\"\"Clean up cache to maintain original knowledge\"\"\"\n",
        "        for i in range(len(cache.key_cache)):\n",
        "            cache.key_cache[i] = cache.key_cache[i][:, :, :origin_len, :]\n",
        "            cache.value_cache[i] = cache.value_cache[i][:, :, :origin_len, :]\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    config = CAGConfig(\n",
        "        max_cache_size=1000,\n",
        "        similarity_threshold=0.85,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        top_k_cache=5,\n",
        "        chunk_size=512\n",
        "    )\n",
        "\n",
        "    cag = EnhancedCAG(\n",
        "        model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # Process document\n",
        "    document_cache = cag.process_document(\"document.txt\")\n",
        "\n",
        "    # Ask questions\n",
        "    questions = [\n",
        "        \"What is the main topic of this document?\",\n",
        "        \"What are the key findings?\"\n",
        "    ]\n",
        "\n",
        "    for question in questions:\n",
        "        answer = cag.answer_question(question, document_cache)\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"A: {answer}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}